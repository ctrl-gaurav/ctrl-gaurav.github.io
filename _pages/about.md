---
permalink: /
title: "Welcome to Gaurav's Homepage!"
excerpt: "About me"
author_profile: true
redirect_from:
 - /about/
 - /about.html
---

<br>

**Note:** *This webpage was last updated on **05/02/2025.***

## About me

Hi folks, welcome to my personal homepage! I'm a first-year MS (thesis) student in Computer Science at [Virginia Tech](https://vt.edu/), and fortunately advised by [Dr. Xuan Wang](https://xuanwang91.github.io/). I am also affiliated with the [Sanghani Center for Artificial Intelligence and Data Analytics](https://sanghani.cs.vt.edu/person/gaurav-srivastava/).

Prior to joining Virginia Tech, I got my Bachelor's degree in Computer Science from Manipal University Jaipur in July 2023. During my Bachelor's program, I was fortunate to be supervised by [Dr. Nitesh Pradhan](https://scholar.google.co.in/citations?hl=en&user=bHEoi4YAAAAJ&view_op=list_works) and worked with [Dr. Vijaypal Singh Dhaka](https://scholar.google.com/citations?user=t9kU8QUAAAAJ&hl=en) and [Dr. Mahesh Jangid](https://scholar.google.co.in/citations?user=ChR5WYcAAAAJ&hl=en). I was also the President's Gold Medalist for Excellence in Research. After that I worked at Dell Technologies for 1 year as a Machine Learning Engineer. Before that, I spent 6 months at Swiggy's Applied Research (Computer Vision) team. 

## Research  Interests

I work on **improving small language models in reasoning**—pushing lightweight LMs to think deeper, act smarter, and collaborate like expert teams. My research spans natural‑language processing, complex reasoning, and model efficiency, all aimed at creating *efficient, low‑cost* AI systems. My current focus areas include:

1. **Complex Reasoning in Small Language Models:**  
  *How far can carefully designed prompting, multi-agent debate, and iterative fine‑tuning push models with only a few billion parameters?* I study emergent reasoning, chain‑of‑thought, and which facets of reasoning are **kept or lost** after compression—revealing **when  and  why** small models **succeed or fail**.

2.  **Multi‑Agent Debate & Self‑Evolution:**  
 I design systems where multiple LMs critique, refine, and distill each other’s outputs. Iteratively fine‑tuning the resulting “debate traces” lets a single model *self‑evolve* without human‑labeled data.

3. **Overthinking in Basic Reasoning:**  
   *Do language models waste cognitive cycles on problems that humans solve almost reflexively?* I also study when language models overthink problems that humans solve instinctively. I developed [**LLMThinkBench**](https://github.com/ctrl-gaurav/LLMThinkBench), a framework that measures when—and why—LLMs *overthink* straightforward math and logical reasoning tasks.

---

<!-- - Natural Language Processing
- Complex Reasoning
- Small Language Models
- Efficient Large Language Models
- Multi-Agent Systems -->

## News

- **[Apr. 22, 2025]** Released the [SLM Reasoning Leaderboard](https://ctrl-gaurav.github.io/slms-reasoning-leaderboard.github.io/)!
- **[Apr. 5, 2025]** Released the [LLMThinkBench](https://github.com/ctrl-gaurav/LLMThinkBench) framework for evaluating basic‑math reasoning and over‑thinking in language models&mdash;install it with `pip install llmthinkbench`!
- **[Feb. 17, 2025]** New [preprint](https://arxiv.org/abs/2502.11569) on the reasoning abilities of small language models.
- **[Oct. 9, 2024]** Accepted a Summer&nbsp;2025 internship offer at [Dell Technologies](https://www.dell.com/en-us) as an AI Research Intern in the Global Office of the CTO (Round Rock, TX)!
- **[Sep. 4, 2024]** Joined [Wang’s Group](https://xuanwang91.github.io/lab) to work on reasoning, small language models, and large language models!
- **[Aug. 6, 2024]** Began my M.S. in Computer Science at Virginia Tech!


<!-- ## News

- **(22/4/25)** We release [SLM Reasoning Leaderboard](https://ctrl-gaurav.github.io/slms-reasoning-leaderboard.github.io/)!
- **(5/4/25)** Released [LLMThinkBench](https://github.com/ctrl-gaurav/LLMThinkBench) framework for evaluating basic math reasoning and overthinking in language models. Check it out here [**pip install llmthinkbench**](https://pypi.org/project/llmthinkbench/)!
- **(17/2/25)** New [preprint](https://arxiv.org/abs/2502.11569) on reasoning ability of small language models!
- **(10/9/24)** Got summer'25 internship offer to join [Dell Technologies](https://www.dell.com/en-us) as an **AI Research Intern** at the Global Office of the **CTO** (Round Rock, TX)!
- **(9/4/24)** Joined [Wang's Group](https://xuanwang91.github.io/lab). Looking forward to work on Reasoning, small language models and large language models!
- **(8/6/24)** Joined Virginia Tech to start Master's in Computer Science! -->

## Honors and Awards

- 🥇 **President's Gold Medal** for Excellence in Research, Manipal University Jaipur (2023)
- 🥈 **Runner-up**, Dell IT Development Program (ITDP) FY'23 Hackathon, Dell Technologies (2023)
- 🪙 **Ranked 13/473** globally in Bitgrit Generative AI Competition, Bitgrit (2023)
- 🪙 **117/26,008**, Amazon ML Challenge 2023, Amazon (2023)
- 🥇 **Three-time recipient** of the Student Excellence Award for publishing research, MUJ (2022 - 2023)
- 🥇 **Best Research Project**, Computer Science Department, Manipal University Jaipur (2022)
- 🥉 **All India Grand Finalist**, Precision Health Challenge 2021-22 Hackathon, Wipro GE Healthcare (2022)
- 🥉 **All India Grand Finalist**, India Automobile Hackathon, NEC and Mitsubishi (2022)
- 🥉 **All India Grand Finalist**, HACKBATTLE: Impact Through Data Hackathon, T-Systems (2022)
- 🥉 **3rd Position, "Hack2Hire"** Hackathon, Dell Technologies (2021)
- 🥇 **Best Senior Hack**, NPSiHacks, Devfolio (2021)
- 🪙 **Kaggle 3X Expert** (Top 20% in Competitions, Top 1% in Titanic, Digit Recognizer) (2020 - 2023)

